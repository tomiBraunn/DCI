<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Camera Comparison</title>
</head>
<body>
    <video id="video" autoplay playsinline></video>
    <canvas id="overlayCanvas"></canvas>
    <button id="captureButton" class="glass">Take Photo</button>
    <img id="capturedImage" style="display: none;"/>

    <input type="file" id="imageUpload" accept="image/*" />
    <img id="uploadedImage" style="display: none;"/>

    <button id="compareButton">Compare Images</button>

    <!-- Result Section -->
    <div id="resultMessage"></div>
    <div id="timeTaken"></div>

    <script src="./face-api.min.js"></script>
    <script>
        const video = document.getElementById("video");
        const overlayCanvas = document.getElementById("overlayCanvas");
        const imageUpload = document.getElementById("imageUpload");
        const uploadedImage = document.getElementById("uploadedImage");
        const capturedImage = document.getElementById("capturedImage");
        const captureButton = document.getElementById("captureButton");
        const compareButton = document.getElementById("compareButton");
        const resultMessage = document.getElementById("resultMessage");
        const timeTaken = document.getElementById("timeTaken");

        let uploadedFaceData;
        let capturedFaceData;

        // Load models
        (async () => {
            await faceapi.nets.ssdMobilenetv1.loadFromUri('./models');
            await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
            await faceapi.nets.faceRecognitionNet.loadFromUri('./models');
            startVideo();
        })();

        // Start the camera
        function startVideo() {
            navigator.mediaDevices
                .getUserMedia({ video: true })
                .then((stream) => {
                    video.srcObject = stream;
                })
                .catch((err) => {
                    console.error("Error accessing the camera: ", err);
                    alert("Could not access the camera.");
                });
        }

        // Handle image upload and face detection
        imageUpload.addEventListener("change", async (e) => {
            const file = e.target.files[0];
            if (file) {
                const img = await faceapi.bufferToImage(file);
                uploadedImage.src = img.src;
                uploadedFaceData = await faceapi
                    .detectSingleFace(img)
                    .withFaceLandmarks()
                    .withFaceDescriptor();

                if (!uploadedFaceData) {
                    alert('No face detected in the uploaded image.');
                }
            }
        });

        // Capture photo from video feed
        captureButton.addEventListener("click", async () => {
            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0, canvas.width, canvas.height);
            const imgDataUrl = canvas.toDataURL('image/jpeg');
            capturedImage.src = imgDataUrl;

            const img = await faceapi.fetchImage(imgDataUrl);
            capturedFaceData = await faceapi
                .detectSingleFace(img)
                .withFaceLandmarks()
                .withFaceDescriptor();

            if (!capturedFaceData) {
                alert('No face detected in the captured image.');
            }
        });

        // Compare faces using Euclidean distance
        compareButton.addEventListener("click", async () => {
            const startTime = performance.now();

            if (!uploadedFaceData || !capturedFaceData) {
                alert("Both images need to be detected for comparison.");
                return;
            }

            // Calculate the Euclidean distance between the two face descriptors
            const distance = faceapi.euclideanDistance(uploadedFaceData.descriptor, capturedFaceData.descriptor);
            console.log("Distance between descriptors: ", distance);

            // Define a threshold (0.6 is a common threshold for face matching)
            const threshold = 0.4;
            const isSamePerson = distance < threshold;
            console.log("Is same person: ", isSamePerson);

            const endTime = performance.now();
            const timeDiff = ((endTime - startTime) / 1000).toFixed(2);

            // Display results
            resultMessage.textContent = isSamePerson
                ? "Son la misma persona."
                : "No son la misma persona.";
            timeTaken.textContent = `Tiempo tomado para identificar: ${timeDiff} segundos.`;
        });
    </script>
</body>
</html>